{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "257f5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#Save a current version of HTML each time scrape so can do a div comparison whenever this script breaks to see how their html has changed\n",
    "# Might be able to replace the sleep for 5 seconds part of the scrape function with webdriver wait, which wsits for javascript to load so dont have to have extra buffer time on each scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5752e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "historical-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run thredup_fullscrape.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "optimum-immune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.12 | packaged by conda-forge | (default, Jan 30 2022, 23:13:55) \n",
      "[Clang 11.1.0 ] on darwin\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> \n",
      "KeyboardInterrupt\n",
      ">>> \n",
      ">>> Collecting tqdm\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "     |████████████████████████████████| 78 kB 6.2 MB/s             \n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.2\n"
     ]
    }
   ],
   "source": [
    "!/usr/bin/env python3\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a5856709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from alive_progress import alive_bar\n",
    "import re\n",
    "import requests # to get image from the web\n",
    "import shutil # to save it locally\n",
    "from os.path import exists\n",
    "from os.path import expanduser\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdfd6c2",
   "metadata": {},
   "source": [
    "# CURRENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469d242",
   "metadata": {},
   "source": [
    "#NOTE TO SELF: If we want 10,000 items from four categories (shirts, sweaters, dresses, outwerwear), we need about 2,500 per category\n",
    "\n",
    "so set max_products = 2,500\n",
    "max_pages = 21 (bc 120 items per pages as default)\n",
    "\n",
    "* For each front_page_url, make sure to save a copy of the link used here and it should have items ordered from newest to oldest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d05ad08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"/Users/karina/Desktop/FashionSave/\"\n",
    "max_products = 3 #set to \"all\" if want to take all products on scraped pages\n",
    "front_page_url = \"https://www.thredup.com/women/outerwear?category_tags=outerwear&department_tags=women&sort=newest_first&page=1\"\n",
    "max_pages = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f7063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "845d19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPE PAGE FUNCTION\n",
    "\n",
    "def scrape_page(my_url):\n",
    "    print(\"scraping url\")\n",
    "    firefox_options = webdriver.FirefoxOptions()\n",
    "    #firefox_options.add_argument('--headless')\n",
    "    firefox_options.add_argument('window-size=1920x1080');\n",
    "    driver = webdriver.Firefox(options=firefox_options)\n",
    "    driver.get(my_url)\n",
    "    time.sleep(10)\n",
    "    page_contents=driver.page_source\n",
    "    page_contents = BeautifulSoup(page_contents, 'html.parser')\n",
    "    driver.quit()\n",
    "    return page_contents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "pleased-hello",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KSN: Page 1...\n",
      "scraping url\n",
      "Product links found\n"
     ]
    }
   ],
   "source": [
    "#GET LINKS TO INDIVIDUAL ITEM PAGES FROM MAIN PAGE\n",
    "\n",
    "def get_item_links(front_page_url, max_pages = 1):\n",
    "    product_links = []\n",
    "    # Everytime range increases, items increase by 50.\n",
    "    for page_number in range(1, max_pages + 1):\n",
    "        print(f\"KSN: Page {page_number}...\")\n",
    "        #See if page exists\n",
    "        try:\n",
    "            url_page = front_page_url + \"&page=\" + str(page_number)\n",
    "            main_page_items = scrape_page(url_page)\n",
    "        except:\n",
    "            print('exception')\n",
    "            break #exit for loop if a page doesn't exist (presumably means past last page for this item) \n",
    "\n",
    "        #Pull all href links\n",
    "        url_front = \"https://www.thredup.com\"\n",
    "        hrefs = []\n",
    "        product_list = []\n",
    "        all_products = main_page_items.find_all(attrs={\"data-inp-label\": \"link-item-card\"})\n",
    "\n",
    "\n",
    "        for product in all_products: #get all product links\n",
    "            product_link = url_front + product[\"href\"]\n",
    "            product_links.append(product_link)\n",
    "            \n",
    "        return product_links\n",
    "\n",
    "            \n",
    "product_links = get_item_links(front_page_url, max_pages)\n",
    "\n",
    "\n",
    "print(f\"Product links found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a002f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Product Id from product page (prepare function)\n",
    "\n",
    "def get_product_id(product_link):\n",
    "    product_id = product_link.split(\"?query\")[0]   \n",
    "    product_id = product_id.split(\"/\")[-1]\n",
    "    \n",
    "    return product_id\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0cc52bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get product image from product page (prepare functions)\n",
    "\n",
    "def get_image_link(scraped_page):\n",
    "\n",
    "    imageLinks = []\n",
    "    images = scraped_page.findAll(\n",
    "        \"img\", {\"class\": lambda L: L and L.startswith(\"u-rounded-4 u-cursor-pointer\")} \n",
    "    )\n",
    "        \n",
    "    image_link = images[0].get(\"src\")\n",
    "    print(image_link)\n",
    "    return image_link\n",
    "\n",
    "def save_image(image_link, product_id, save_folder):\n",
    "    filename = save_folder + \"item\" + product_id +\".jpg\"\n",
    "\n",
    "    # Open the url image, set stream to True, this will return the stream content.\n",
    "    r = requests.get(image_link, stream = True)\n",
    "\n",
    "     # Check if the image was retrieved successfully\n",
    "    if r.status_code == 200:\n",
    "        # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
    "        r.raw.decode_content = True\n",
    "        # Open a local file with wb ( write binary ) permission.\n",
    "        with open(filename,'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "        print(\"image saved\")\n",
    "    else:\n",
    "        print('Image Couldn\\'t be retrieved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "93635c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_and_brand(scraped_page):\n",
    "    size = scraped_page.find_all(\"div\", attrs={\"class\": \"u-text-16\"}) #structure of find_all input is (element, attrs={})\n",
    "    brand = size[0].parent.find_previous_sibling().find(\"a\")[\"title\"]\n",
    "    size = size[0].text\n",
    "    print(size)\n",
    "    print(brand)\n",
    "    return size, brand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "675e31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(scraped_page):\n",
    "    category = scraped_page.find_all(\"span\", attrs={\"class\": \"u-text-16\"}) #structure of find_all input is (element, attrs={})\n",
    "    category = category[0].text\n",
    "    print(category)\n",
    "    return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "070eadd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_price(scraped_page):\n",
    "    price = scraped_page.find_all(\"span\", attrs={\"class\": \"price\"}) \n",
    "    price = price[0].text\n",
    "    print(price)\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "dc72a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condition(scraped_page):\n",
    "    condition_header = scraped_page.find(\"h2\", string=\"Condition\")\n",
    "    condition = condition_header.find_next_sibling()\n",
    "    condition = condition.text\n",
    "    print(condition)\n",
    "    return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "893af627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_material_and_features(scraped_page):\n",
    "    details_header = scraped_page.find(\"h2\", string=\"Item details\")\n",
    "    details = details_header.parent.find_next_sibling().find_all('li')\n",
    "    material = details[0].text\n",
    "    features = details[1].text\n",
    "    \n",
    "    print(material)\n",
    "    print(features)\n",
    "\n",
    "    return material, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4750cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measurements_and_fit(scraped_page):\n",
    "    try:\n",
    "        measurements_and_fit_header = scraped_page.find(\"h2\", string=\"Size & fit\")\n",
    "        measurements_and_fit = measurements_and_fit_header.find_next_sibling().find_all('li')\n",
    "        measurements = measurements_and_fit[0].text\n",
    "        measurements = measurements.replace(\"How we measure\",\"\")\n",
    "        fit = measurements_and_fit[1].text\n",
    "    except:\n",
    "        measurements = None\n",
    "        fit = None\n",
    "    \n",
    "    print(measurements)\n",
    "    print(fit)\n",
    "\n",
    "    return measurements, fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b6216f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all product info from product page\n",
    "\n",
    "def get_product_info(product_link, save_folder):\n",
    "    \n",
    "    scraped_page = scrape_page(product_link)\n",
    "\n",
    "    product_id = get_product_id(product_link)\n",
    "       \n",
    "    #Get and Save Image\n",
    "    image_link = get_image_link(scraped_page)\n",
    "    save_image(image_link, product_id, save_folder)\n",
    "    \n",
    "    #Get Size\n",
    "    size, brand = get_size_and_brand(scraped_page)\n",
    "    \n",
    "    #Get item type\n",
    "    category = get_category(scraped_page)\n",
    "    \n",
    "    #Get Condition\n",
    "    condition = get_condition(scraped_page)\n",
    "    \n",
    "    #Get Material and Features\n",
    "    material, features = get_material_and_features(scraped_page)\n",
    "    \n",
    "    #Get Size and Fit\n",
    "    measurements, fit = get_measurements_and_fit(scraped_page)\n",
    "    \n",
    "    \n",
    "    product_dict = {\n",
    "        \"product_link\" : product_link,\n",
    "        \"product_id\" : product_id,\n",
    "        \"size\" : size,\n",
    "        \"brand\" : brand,\n",
    "        \"category\" : category,\n",
    "        \"condition\" : condition,\n",
    "        \"material\" : material,\n",
    "        \"features\" : features,\n",
    "        \"measurements\" : measurements,\n",
    "        \"fit\" : fit\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    return product_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b66c3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting product info\n",
      "scraping url\n",
      "https://cf-assets-thredup.thredup.com/assets/556351003/xlarge.jpg\n",
      "image saved\n",
      "Size 4\n",
      "Kay Unger\n",
      "Jacket\n",
      "excellent\n",
      "56% cotton, 22% polyamide, 20% viscose, 2% polyester\n",
      "3/4 sleeve, jacket, marled, frayed edges, fringe & frayed accents, multi color, white, cropped top\n",
      "None\n",
      "None\n",
      "getting product info\n",
      "scraping url\n",
      "https://cf-assets-thredup.thredup.com/assets/549052378/xlarge.jpg\n",
      "image saved\n",
      "Size XXS\n",
      "Vince.\n",
      "Jacket\n",
      "very good\n",
      "36% merino wool, 26% nylon, 18% alpaca, 18% viscose, 2% spandex\n",
      "Long sleeve, coat, solid, black, knee length\n",
      "None\n",
      "None\n",
      "getting product info\n",
      "scraping url\n",
      "https://cf-assets-thredup.thredup.com/assets/502740580/xlarge.jpg\n",
      "image saved\n",
      "Size M\n",
      "Athleta\n",
      "Track Jacket\n",
      "very good\n",
      "100% polyester\n",
      "Solid, zipper detail, metal accents, black, classic\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#PRODUCT LOOP TO GET ALL OF EACH PRODUCT'S INFO\n",
    "\n",
    "if max_products == \"all\":\n",
    "    max_products = len(product_links)\n",
    "\n",
    "products = []\n",
    "for product_link in tqdm(product_links[:max_products]):\n",
    "    print(f\"--------------\\ngetting info for product: {product_link}\")\n",
    "    product = get_product_info(product_link, save_folder)\n",
    "    products.append(product)\n",
    "    #Pause for random duration to not trigger bot blocker\n",
    "    time.sleep(random.randrange(5, 10))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4bcbdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save json\n",
    "\n",
    "# Specify the filename\n",
    "filename = 'scraped_product_details.json'\n",
    "\n",
    "with open(save_folder + filename, 'w') as f:\n",
    "    json.dump(products, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ab1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FashionProjectVenv",
   "language": "python",
   "name": "fashionprojectvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
